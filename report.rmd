---
Author:  RandomM53
output:
  html_document:
    theme: readable
    toc: true
  pdf_document: default
title: Factors Impacting Life Expectancy
urlcolor: cyan
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: inline
---

# Introduction

This project will research factors that might predict life expectancy
across multiple regions globally. It is generally accepted that
developed countries have lower mortality and higher lifespan than their
less-developed peers. But how true is this assumption? And if these
countries do indeed have lower mortality, which factors are most
responsible?

Developed countries tend to have more resources, better nutrition
standards and access to advanced medical infrastructure, all of which
facilitate longevity. However, it shouldn't be assumed that all aspects
of development are positive. Many developed countries are more likely to
suffer from higher rates of obesity or alcoholism, counteracting their
advantages in nutrition and medicine. Moreover, the complex economies of
developed countries may result in more stressful lifestyles, further
harming mortality.

Buddhist monks live in Nepal don't spend their sleeping time on swiping
phones, breath car exhaust, or eat packaged food full of chemicals. It's
true Nepal's GDP is far behind US, but do US residents live longer than
Nepal people necessarily? Hopefully data analysis with this data set can
shed some light.

## Description of the data set

For this project we propose using the `Life Expectancy (WHO)` data set
which is available
[here](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who).
The data is an amalgamation of two sources. The primary source is The
Global Health Observatory (GHO) repository, which is maintained by the
World Health Organization (WHO) for the purpose of tracking demographic
and health factors impacting mortality. The data spans 193 countries and
15 years, from 2000 to 2015. This dataset is supplemented with United
Nations economic data. The resulting dataset will enable us to assess
the impact of immunization, economic and social factors on mortality.

The dataset was compiled by Deeksha Russel and Duan Wang. It is a
curated subset of the Global Health Observatory database, with only
those variables deemed most critical and representative being retained
in the file dataset. The dataset has also been cleaned to remove
countries for which data was incomplete.

```{r}
library(readr)
data = read_csv('./data/life_expectancy.csv', show_col_types = FALSE)
head(data)
```

There are 22 columns and 2938 records in the CSV file. `Life expectancy`
will be used as the response variable, and the rest are predictor
candidates. The table below lists out some important ones,

```{r}
library(knitr)
predictors = data.frame(names = c("country", "status", "adult mortality", "infant deaths", "Alcohol", "BMI", "GDP", "Population", "Schooling"),
                        types = c("categorical", "dummy", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric"))
kable(predictors)
```

## Column definitions

| Name                   | Definition                                                                                             |
|-----------------|-------------------------------------------------------|
| country                | country                                                                                                |
| year                   | year                                                                                                   |
| status                 | Developed or Developing                                                                                |
| life expectancy        | life expectancy in age                                                                                 |
| adult mortality        | Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population) |
| infant deaths          | Number of Infant Deaths per 1000 population                                                            |
| alcohol                | Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)                             |
| percentage expenditure | Expenditure on health as a percentage of Gross Domestic Product per capita (%)                         |
| Hepatitis B            | Hepatitis B (HepB) immunization coverage among 1-year-olds (%)                                         |
| Measles                | Measles - number of reported cases per 1000 population                                                 |
| BMI                    | Average Body Mass Index of entire population                                                           |
| under-five deaths      | Number of under-five deaths per 1000 population                                                        |
| Polio                  | Polio (Pol3) immunization coverage among 1-year-old (%)                                                |
| total expenditure      | General government expenditure on health as a percentage of total government expenditure (%)           |
| Diphtheria             | Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)             |
| HIV/AIDS               | Deaths per 1 000 live births HIV/AIDS (0-4 years)                                                      |
| GDP                    | Gross Domestic Product per capita (in USD)                                                             |
| Population             | Population of the country                                                                              |
| thinness 1-19 years    | Prevalence of thinness among children and adolescents for Age 10 to 19 (% )                            |
| thinness 5-9 years     | Prevalence of thinness among children for Age 5 to 9(%)                                                |
| income composition     | Human Development Index in terms of income composition of resources (index ranging from 0 to 1)        |
| schooling              | Number of years of Schooling(years)                                                                    |

# Methods

## Data Cleaning

Original variable names are not r-friendly. We will rename them to
follow snake case for easy access on further analysis. - rename
variables - convert category variables to factors - filter records based
on column values - omit NA values

```{r}
names(data)[names(data) == "Country"] = "country"
names(data)[names(data) == "Year"] = "year"
names(data)[names(data) == "Status"] = "status"
names(data)[names(data) == "Life expectancy"] = "life_expectancy"
names(data)[names(data) == "Adult Mortality"] = "adult_mortality"
names(data)[names(data) == "infant deaths"] = "infant_deaths"
names(data)[names(data) == "Alcohol"] = "alcohol"
names(data)[names(data) == "percentage expenditure"] = "percent_exp"
names(data)[names(data) == "Hepatitis B"] = "hepatitis"
names(data)[names(data) == "measles"] = "measles"
names(data)[names(data) == "under-five deaths"] = "under_5_deaths"
names(data)[names(data) == "Polio"] = "polio"
names(data)[names(data) == "Total expenditure"] = "total_exp"
names(data)[names(data) == "Diphtheria"] = "diphtheria"
names(data)[names(data) == "HIV/AIDS"] = "hiv_aids"
names(data)[names(data) == "Population"] = "population"
names(data)[names(data) == "thinness  1-19 years"] = "thinness_1_19"
names(data)[names(data) == "thinness 5-9 years"] = "thinness_5_9"
names(data)[names(data) == "Income composition of resources"] = "HBI"
names(data)[names(data) == "Schooling"] = "schooling"

data$country = as.factor(data$country)
data$year = as.factor(data$year)
data$status = as.factor(data$status)

data = subset(data, percent_exp > 0)
data = subset(data, hiv_aids > 0)
data = subset(data, GDP > 0)
data = na.omit(data)
```

## Identify Potential Transformations and Collinearity

For this analysis we intend to perform linear regression with multiple
predictors. However, we need to verify that our predictors are
independent, which is a core assumption of the linear regression model.
If multiple predictors in our model are correlated, it will hamper our
ability to determine the impact of each predictor on the response
variable, since any change to the collinear predictors will exert an
outsized influence on the response variable. Colliwnear variables will
also increase error and reduce the predictive power of the model.

First, we can assess the data visually by creating a matrix of
scatterplots to check the shapes of curves for -
`life_expectancy vs numeric predictors` - $Predictor_x$ vs $Predictor_y$
. Using the pairs function, we can plot all possible scatterplots
between pairs of response and predictor variables in the dataset.

```{r}
data_numeric_only = subset(data, select = -c(country, year, status))
pairs(data_numeric_only)
```

From above matrix of scatterplots, we can observe - collinearity -
`infant_death` and `under_5_death` - `thinness_5_9` and
`thinness_1_19` - predictors with non-linear shape of curve against life
expectancy - `percent_exp` - `hiv_aids` - `GDP`

Additionally, we can employ a numerical methodology to further check for
collinearity. We can do this by using the cor() function to check for
all pairwise correlations between numerical variables in the dataset.

```{r}
var_cors = round(cor(data_numeric_only), 2)
var_cors
```

This analysis can be further refined to identify the variable pairs with
the highest correlations. This will enable us to detect significant
collinearities with the potential to undermine our model.

```{r}
var_cors_df = as.data.frame(as.table(var_cors))
var_cors_df_ordered = var_cors_df[order(var_cors_df$Freq, decreasing = TRUE), ]
var_cors_df_ordered = subset(var_cors_df_ordered, var_cors_df_ordered$Var1 != var_cors_df_ordered$Var2)
var_cors_df_ordered = subset(var_cors_df_ordered, (var_cors_df_ordered$Var1 != "life_expectancy" & var_cors_df_ordered$Var2 != "life_expectancy"))
colnames(var_cors_df_ordered) = c("response1", "response2", "cor")
var_cors_df_ordered
```

From the above, it is evident that many of the predictor pairs in our
model exhibit significant colinearity. This is a result of many
variables in the dataset having some level of overlap – such as under 5
deaths and infant deaths. We will avoid using these concurrently in the
model.

Next, we can search for evidence of collinearity by examining some of
the coefficient parameters of the complete additive model.

```{r}

#citation for paste(): https://stackoverflow.com/questions/58349721/how-do-i-use-paste-and-as-formula-to-generate-a-complex-formula

full_model = lm(life_expectancy ~ ., data = data)
summary(full_model)$r.squared

predictors = names(data)
num_predictors = length(predictors)
results = rep(0, num_predictors)
for (i in 1:num_predictors) {
  predictor = predictors[i]
  temp_analysis_model = as.formula(paste("life_expectancy ~ . -", predictor))
  temp_analysis_model = lm(temp_analysis_model, data = data)                      
  r_squared = summary(temp_analysis_model)$r.squared
  results[i] = r_squared
}
results_df = data.frame(
  var = predictors,
  result = results
)
```

```{r}
results_df = results_df[order(results_df$result, decreasing = TRUE),]
results_df
```

Finally, we can calculate VIF (variance inflation factor) for each
parameter of the model. VIF is a useful measure for assesing
collinearity because it attempts to isolate the impact of collinearity
on model variance. VIF greater than 5 generally indicates the presence
of collinearity

```{r}
library(car)
fit_add_orign = lm(life_expectancy ~ ., data = subset(data, select = -c(country, year)))
vif_reuslts_table = data.frame(as.table(vif(fit_add_orign)))
colnames(vif_reuslts_table) = c("predictor", "VIF")
vif_reuslts_table
```

We want to mitigate the impact of collinearity on our model so they
don't impact our results. To do this, we first perform a logarithmic
adjustment for those predictors with non-linear curves.

```{r}
{r}
data_transformed = data.frame(data)
data_transformed["log_percent_exp"] = log(data_transformed$percent_exp)
data_transformed["log_hiv_aids"] = log(data_transformed$hiv_aids)
data_transformed["log_GDP"] = log(data_transformed$GDP)
```

Next, we will need to prune our model to remove predictors with high
collinearity. We remove linearly correlated columns and two category
columns - `country` and `year`. remove `thinness_5_9` and `infant_death`
because they are subsets to `thiness_1_19` and `under_5_deaths`
respectively - remove `year` to get rid of time series characteristic -
remove `country` to mitigate geographic influence - omit NA values

```{r}
data_cleaned = subset(data_transformed, select = -c(country, year, percent_exp, hiv_aids, GDP, thinness_5_9, infant_deaths))
data_cleaned = na.omit(data_cleaned)
```

## Model Selection

### compare additive models with predictors and transformed predictors

```{r}
fit_add_orign = lm(life_expectancy ~ ., data = subset(data, select = -c(country, year)))
summary(fit_add_orign)
fit_add_trans = lm(life_expectancy ~ ., data = data_cleaned)
summary(fit_add_trans)
```

Both models are significant under 0.01 significant level, and the one
with transformed data has higher `r-squared` value. Further optimization
will be based the data set `data_cleaned`.

### perform a regression with level 2 interaction

```{r}
fit_int = lm(life_expectancy ~ .^2, data = data_cleaned)
```

### compare the interaction model with additive model

```{r}
anova(fit_add_trans, fit_int)
```

`p-value` is small enough to reject the hypothesis that interactive
predictors were in-significant, hence interactive model `fit_int` is
preferred over the additive one.

### backward step-wise selection with AIC

```{r}
fit_selected = step(fit_int, direction = "backward", trace = 0)
summary(fit_selected)
```

Noticed that not all selected predictors are significant with a high
significance level such as 99%. Try to fit a simple model which excludes
predictors with `p-value > 0.01`

```{r}
df_coef = data.frame(summary(fit_selected)$coefficients)
row.names(df_coef)[df_coef["Pr...t.."] < 0.01]
(predictor_formula_str = paste(row.names(df_coef)[df_coef["Pr...t.."] < 0.01][-1], collapse = " + "))
fit_small = lm(life_expectancy ~ adult_mortality + Measles + BMI + polio + total_exp + population + thinness_1_19 + HBI + log_hiv_aids + status:adult_mortality + status:hepatitis + status:total_exp + status:thinness_1_19 + status:HBI + status:schooling + adult_mortality:BMI + adult_mortality:schooling + adult_mortality:log_hiv_aids + adult_mortality:log_GDP + alcohol:hepatitis + alcohol:Measles + alcohol:log_percent_exp + alcohol:log_hiv_aids + hepatitis:total_exp + hepatitis:log_percent_exp + BMI:population + BMI:thinness_1_19 + BMI:schooling + under_5_deaths:polio + under_5_deaths:HBI + under_5_deaths:schooling + polio:HBI + total_exp:thinness_1_19 + total_exp:HBI + diphtheria:HBI + diphtheria:schooling + thinness_1_19:HBI + thinness_1_19:schooling + thinness_1_19:log_hiv_aids + HBI:schooling + HBI:log_hiv_aids + schooling:log_percent_exp + schooling:log_GDP + log_percent_exp:log_GDP,
               data = data_cleaned)
anova(fit_small, fit_selected)
```

`fit_selected` is preferred over the smaller model on anova analysis,
but how about the Test-Train Split?

```{r, warning=FALSE}
set.seed(233)
total = nrow(data_cleaned)
trn_idx = sample(total, floor(total * 0.8))
data_trn = data_cleaned[trn_idx, ]
data_tst = data_cleaned[-trn_idx, ]

rmse <- function(model, new_data) {
  y_hat = predict(model, newdata = new_data)
  return(sqrt(mean((y_hat - data_tst$life_expectancy)^2)))
}

fit_complex = lm(life_expectancy ~ status + adult_mortality + alcohol + hepatitis + 
    Measles + BMI + under_5_deaths + polio + total_exp + diphtheria + 
    population + thinness_1_19 + HBI + schooling + log_percent_exp + 
    log_hiv_aids + log_GDP + status:adult_mortality + status:alcohol + 
    status:hepatitis + status:BMI + status:total_exp + status:thinness_1_19 + 
    status:HBI + status:schooling + adult_mortality:BMI + adult_mortality:population + 
    adult_mortality:schooling + adult_mortality:log_hiv_aids + 
    adult_mortality:log_GDP + alcohol:hepatitis + alcohol:Measles + 
    alcohol:BMI + alcohol:population + alcohol:HBI + alcohol:schooling + 
    alcohol:log_percent_exp + alcohol:log_hiv_aids + hepatitis:Measles + 
    hepatitis:total_exp + hepatitis:population + hepatitis:HBI + 
    hepatitis:log_percent_exp + Measles:BMI + Measles:under_5_deaths + 
    Measles:total_exp + Measles:HBI + Measles:log_percent_exp + 
    Measles:log_hiv_aids + Measles:log_GDP + BMI:under_5_deaths + 
    BMI:total_exp + BMI:diphtheria + BMI:population + BMI:thinness_1_19 + 
    BMI:HBI + BMI:schooling + under_5_deaths:polio + under_5_deaths:total_exp + 
    under_5_deaths:population + under_5_deaths:HBI + under_5_deaths:schooling + 
    under_5_deaths:log_percent_exp + under_5_deaths:log_hiv_aids + 
    under_5_deaths:log_GDP + polio:total_exp + polio:population + 
    polio:HBI + total_exp:diphtheria + total_exp:thinness_1_19 + 
    total_exp:HBI + total_exp:log_percent_exp + total_exp:log_GDP + 
    diphtheria:HBI + diphtheria:schooling + diphtheria:log_percent_exp + 
    population:HBI + population:log_percent_exp + population:log_hiv_aids + 
    population:log_GDP + thinness_1_19:HBI + thinness_1_19:schooling + 
    thinness_1_19:log_hiv_aids + thinness_1_19:log_GDP + HBI:schooling + 
    HBI:log_hiv_aids + schooling:log_percent_exp + schooling:log_hiv_aids + 
    schooling:log_GDP + log_percent_exp:log_GDP,
    data = data_trn)
fit_simple = lm(life_expectancy ~ adult_mortality + Measles + BMI + polio + total_exp + 
    population + thinness_1_19 + HBI + log_hiv_aids + status:adult_mortality + 
    status:hepatitis + status:total_exp + status:thinness_1_19 + 
    status:HBI + status:schooling + adult_mortality:BMI + adult_mortality:schooling + 
    adult_mortality:log_hiv_aids + adult_mortality:log_GDP + 
    alcohol:hepatitis + alcohol:Measles + alcohol:log_percent_exp + 
    alcohol:log_hiv_aids + hepatitis:total_exp + hepatitis:log_percent_exp + 
    BMI:population + BMI:thinness_1_19 + BMI:schooling + under_5_deaths:polio + 
    under_5_deaths:HBI + under_5_deaths:schooling + polio:HBI + 
    total_exp:thinness_1_19 + total_exp:HBI + diphtheria:HBI + 
    diphtheria:schooling + thinness_1_19:HBI + thinness_1_19:schooling + 
    thinness_1_19:log_hiv_aids + HBI:schooling + HBI:log_hiv_aids + 
    schooling:log_percent_exp + schooling:log_GDP + log_percent_exp:log_GDP,
    data = data_trn)
trn_rmse_complex = rmse(fit_complex, data_trn)
tst_rmse_complex = rmse(fit_complex, data_tst)
trn_rmse_simple = rmse(fit_simple, data_trn)
tst_rmse_simple = rmse(fit_simple, data_tst)

library(knitr)
kable(data.frame(model = c("Simple", "Complex"),
                 "Train RMSE" = c(trn_rmse_simple, trn_rmse_complex),
                 "Test RMSE" = c(tst_rmse_simple, tst_rmse_complex)))
```

The simple model has lower RMSE on both train and test data sets. Simple
model will be selected on further analysis.

## Model Diagnostics
#### @Jaccob


## Setup
{r, include=FALSE}
# Set default CRAN mirror and install required packages
options(repos = c(CRAN = "https://cloud.r-project.org"))

required_packages <- c("tseries", "lmtest", "car", "glmnet", "caret")
new_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if (length(new_packages)) install.packages(new_packages, dependencies = TRUE)


## Residuals Analysis
{r, echo=FALSE}
# Residual Plots for Complex Model
par(mfrow = c(2, 2))
plot(fit_complex)


### Analysis & Conclusion
- **Residuals vs Fitted:** Residuals are scattered somewhat randomly, supporting linearity, though deviations exist that warrant further investigation.
- **Q-Q Plot:** Residuals follow the theoretical quantile line with mild deviations in the tails, suggesting approximate normality with possible outliers.
- **Scale-Location Plot:** The spread of residuals appears consistent, though slight fanning at higher fitted values indicates mild heteroscedasticity.
- **Residuals vs Leverage:** Points with high leverage are visible, with one exceeding Cook's distance. These points require further evaluation.

---

## Additional Diagnostic Plots

### Cook's Distance Plot
{r, echo=FALSE}
# Cook's Distance Plot
cooksD <- cooks.distance(fit_complex)
plot(cooksD, main="Cook's Distance", ylab="Cook's Distance")
abline(h = 4/(nrow(model.matrix(fit_complex))), col="red")


#### Analysis & Conclusion
- **Cook's Distance:** Points above the red line are highly influential and may skew regression results. Investigate these points further to determine if they are outliers or valid influential points. Consider removal or adjustment if they negatively affect the model.

---

## Statistical Tests Summary
{r, include=FALSE}
# Statistical Tests
library(lmtest)
bp_complex <- bptest(fit_complex)
shapiro_complex <- shapiro.test(resid(fit_complex))
dw_complex <- dwtest(fit_complex)


| Test                  | Statistic | p-value         | Conclusion               |
|----------------------|------------|----------------|---------------------------|
| Breusch-Pagan        | r round(bp_complex$statistic, 2)  | r format(bp_complex$p.value, scientific=TRUE) | Heteroscedasticity: Present if p < 0.05 |
| Shapiro-Wilk         | r round(shapiro_complex$statistic, 2) | r format(shapiro_complex$p.value, scientific=TRUE) | Normality: Violated if p < 0.05 |
| Durbin-Watson        | r round(dw_complex$statistic, 2) | r format(dw_complex$p.value, scientific=TRUE) | Autocorrelation: Not Present if p > 0.05 |

### Analysis & Conclusion
- **Breusch-Pagan Test:** Confirms heteroscedasticity if p < 0.05. Apply transformations or robust standard errors.
- **Shapiro-Wilk Test:** Significant p-values suggest non-normality;
- **Durbin-Watson Test:** No evidence of autocorrelation; residuals appear independent.

---

## Multicollinearity Check
{r, include=FALSE}
# VIF Analysis
library(car)
vif_complex <- vif(fit_complex)


| Variable              | VIF Value |
|----------------------|------------|
r paste0("| ", rownames(vif_complex), " | ", round(vif_complex, 2), " |")

### Analysis & Conclusion
- **Observation:** High VIF values indicate severe multicollinearity.
- **Action:** Consider reducing multicollinearity by removing or transforming variables, using Ridge/Lasso regression, or applying PCA.

---

## Model Evaluation Metrics
{r, include=FALSE}
# Model Performance Metrics
adj_r2_complex <- summary(fit_complex)$adj.r.squared
aic_complex <- AIC(fit_complex)
bic_complex <- BIC(fit_complex)


| Metric              | Complex Model |
|---------------------|----------------|
| Adjusted R-squared | r round(adj_r2_complex, 4) |
| AIC                | r round(aic_complex, 2)    |
| BIC                | r round(bic_complex, 2)    |

### Analysis & Conclusion
- **Adjusted R-squared:** Indicates the model’s explanatory power after adjusting for predictors.
- **AIC/BIC:** Lower values suggest a better model fit. Consider reevaluating the model after addressing issues.

## Comprehensive Conclusion
Based on the comprehensive diagnostic evaluation of the model, several critical insights were identified:

1. **Model Fit and Linearity:**
   - The model shows an approximately linear relationship, supported by the residual plots and the Residuals vs Fitted plot. However, deviations in residual patterns and influential points suggest possible specification errors.

2. **Influential Points:**
   - Cook's Distance analysis indicates several highly influential points that exceed the recommended threshold. Further examination and potential adjustments are necessary to improve model robustness.

3. **Assumption Violations:**
   - The Breusch-Pagan test revealed heteroscedasticity, while the Shapiro-Wilk test showed violations of normality. These results suggest a need for variable transformations or robust estimation techniques.

4. **Multicollinearity:**
   - High VIF values point to severe multicollinearity, reducing coefficient reliability. Consider removing or combining highly correlated variables, or applying regularization techniques.

5. **Overall Model Performance:**
   - Despite the identified issues, the Adjusted R-squared value and relatively low AIC/BIC values suggest that the model has reasonable explanatory power but could be improved by addressing the diagnostics above.
